# training settings
seed: 6660857
dataset: CIFAR10
strategy: baseline
batch_size: 256
test-batch-size: 50
epochs: 200
optimization: fedlap

# parameters for dataset condensation
reset_method: current_weight
init: real
reinit_image: True
dsa: True
dsa_strategy: color_crop_cutout_flip_scale_rotate
dc_aug_param: none
dis_metric: gm
reg_type: mse
reg_weight: 0.1
iteration: 1
ipc: 50
batch_loop: 40
outer_loop: 5
inner_loop: 0
boost_loop: 5
lr_img: 100
dsc_server_iter: 100
server_batch_size: 512
eps_ball: 10
truncate: True
truncate_crit: loss
truncate_voting: max

# DP
enable_privacy: False
target_epsilon: 10
target_delta: 0.00001
max_norm: 0.1

# parameters for logging
save-freq: 10
save-model: True
log-interval: 20
save-interval: 5
test-interval: 1

# model settings
arch: ConvNet
lr_net: 0.1 #update the global model (and client models in dsc settings)
momentum: 0 #momentum for the server
weight_decay: 0 #weight_decay for the server
client_settings:
  type: cosine_decay #cosine_decay #multistep #cosine
  lr: 0.1 # update client models when using the baseline
  milestones: [20, 40, 60]
  gamma: 0.2
  T_0: 1
  T_mult: 1
  eta_min: 0.01
  momentum: 0
  weight_decay: 0

# parameter for federated configuration
epoch-client: 5
n-client: 5
n-update-client: 5
shard_per_user: 2
noniid: True